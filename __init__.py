import os
import sys

import torchaudio
import torch
import numpy as np
import librosa

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT_DIR)
sys.path.append(f"{ROOT_DIR}/CosyVoice")
sys.path.insert(0, f"{ROOT_DIR}/CosyVoice/third_party/Matcha-TTS")

from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
import folder_paths

models_root = os.path.join(folder_paths.get_folder_paths("cosyvoice")[0],"pretrained_models")

GLOBAL_CATEGORY = "HJH_CosyVoiceğŸª…"

class CosyVoiceModel:
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰è¾“å…¥å‚æ•°"""
        return {
            "required": {
                "model":([
                    "CosyVoice2-0.5B",
                    "CosyVoice-300M-SFT",
                    "CosyVoice-300M",
                    "CosyVoice-300M-25Hz",
                    "CosyVoice-300M-Instruct",
                ],),
                "fp16": ([False, True],),
            },
        }

    RETURN_TYPES = ("COSYVOICEMODEL",)
    RETURN_NAMES = ("cosyvoice_model",)
    FUNCTION = "load"
    CATEGORY = GLOBAL_CATEGORY

    def __init__(self):
        pass

    def load(self, model, fp16=False):
        if model == "CosyVoice2-0.5B":
            cosyvoice = CosyVoice2(os.path.join(models_root,"CosyVoice2-0.5B"), load_jit=False, load_trt=False, fp16=fp16)
        elif model == "CosyVoice-300M-SFT":
            cosyvoice = CosyVoice(os.path.join(models_root,"CosyVoice-300M-SFT"), load_jit=False, load_trt=False, fp16=fp16)
        elif model == "CosyVoice-300M":
            cosyvoice = CosyVoice(os.path.join(models_root,"CosyVoice-300M"), load_jit=False, load_trt=False, fp16=fp16)
        elif model == "CosyVoice-300M-25Hz":
            cosyvoice = CosyVoice(os.path.join(models_root,"CosyVoice-300M-25Hz"), load_jit=False, load_trt=False, fp16=fp16)
        elif model == "CosyVoice-300M-Instruct":
            cosyvoice = CosyVoice(os.path.join(models_root,"CosyVoice-300M-Instruct"), load_jit=False, load_trt=False, fp16=fp16)

        return cosyvoice,


max_val = 0.8
target_sr = 16000
def postprocess(speech, top_db=60, hop_length=220, win_length=440):
    speech, _ = librosa.effects.trim(
        speech, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    if speech.abs().max() > max_val:
        speech = speech / speech.abs().max() * max_val
    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)
    return speech

def get_output_data(generator):
    chunks = []
    for chunk in generator:
        chunks.append(chunk['tts_speech'].numpy().flatten())
    output = np.array(chunks)
    return torch.from_numpy(output).unsqueeze(0)

def audio_prepare(audio):
    waveform = audio['waveform'].squeeze(0)
    source_sr = audio['sample_rate']
    speech = waveform.mean(dim=0,keepdim=True)
    if source_sr != target_sr:
        speech = torchaudio.transforms.Resample(orig_freq=source_sr, new_freq=target_sr)(speech)

    return postprocess(speech)


class CosyVoiceNode:
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰è¾“å…¥å‚æ•°"""
        return {
            "required": {
                "cosyvoice_model": ("COSYVOICEMODEL",),
                "mode":(["Zero Shot", "Cross Lingual", "Instruct2","SFT", "VC", "Instruct"],),
                "tts_text": ("STRING", {"default":""}),
                "speed":("FLOAT",{"min":0.1,"max":2.0,"default":1.0}),
                "text_frontend":([True,False],),
            },
            "optional":{
                "prompt_audio": ("AUDIO",),
                "source_audio": ("AUDIO",),
                "prompt_text": ("STRING", {"default":""}),
                "instruct_text": ("STRING", {"default":""}),
                "spk_id": (['ä¸­æ–‡å¥³', 'ä¸­æ–‡ç”·', 'æ—¥è¯­ç”·', 'ç²¤è¯­å¥³', 'è‹±æ–‡å¥³', 'è‹±æ–‡ç”·', 'éŸ©è¯­å¥³'],),
            },
        }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("generated_audio",)
    FUNCTION = "generate"
    CATEGORY = GLOBAL_CATEGORY

    OUTPUT_NODE = True

    def __init__(self):
        pass

    def generate(self, cosyvoice_model, mode, tts_text, prompt_audio=None,source_audio=None, prompt_text="",instruct_text="",spk_id="",speed=1.0,text_frontend=False):
        cosyvoice = cosyvoice_model

        is_CosyVoice2 = isinstance(cosyvoice, CosyVoice2)

        # prompt_speech_16k = prompt_wav["waveform"].squeeze(0)

        if prompt_audio is not None:
            prompt_speech_16k = audio_prepare(prompt_audio)

        if source_audio is not None:
            source_speech_16k = audio_prepare(source_audio)

        generator = None
        if mode == "Zero Shot":
            '''
            cosyvoice.inference_zero_shot params:
                tts_text: éœ€è¦åˆæˆè¯­éŸ³çš„æ–‡æœ¬ã€‚
                prompt_text: æä¾›çš„æç¤ºæ–‡æœ¬ï¼Œç”¨äºè¾…åŠ©ç”Ÿæˆè¯­éŸ³ã€‚
                prompt_speech_16k: æä¾›çš„æç¤ºè¯­éŸ³ï¼Œé‡‡æ ·ç‡ä¸º16kHzï¼Œç”¨äºè¾…åŠ©ç”Ÿæˆè¯­éŸ³ã€‚
                stream: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä»¥æµå¼æ–¹å¼è¾“å‡ºè¯­éŸ³ã€‚å¦‚æœè®¾ç½®ä¸º Trueï¼Œåˆ™å‡½æ•°ä¼šé€å—ç”Ÿæˆå¹¶è¿”å›è¯­éŸ³æ•°æ®ã€‚
                speed: æ§åˆ¶ç”Ÿæˆè¯­éŸ³çš„è¯­é€Ÿï¼Œ1.0è¡¨ç¤ºæ­£å¸¸è¯­é€Ÿã€‚é»˜è®¤: 1.0
                text_frontend: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨å‰ç«¯æ–‡æœ¬å¤„ç†æ¨¡å—ã€‚é»˜è®¤: True
            '''
            if is_CosyVoice2:
                generator = cosyvoice.inference_zero_shot(tts_text, prompt_text, prompt_speech_16k, speed=speed, stream=False,text_frontend=text_frontend)
            else:
                generator = cosyvoice.inference_zero_shot(tts_text, prompt_text, prompt_speech_16k, stream=False,)

        elif mode == "Cross Lingual":
            '''
            tts_text, prompt_speech_16k, stream=False, speed=1.0, text_frontend=True
            æ ‡è®°å‚è€ƒ: custom_nodes/ComfyUI_CosyVoice2/CosyVoice/cosyvoice/tokenizer/tokenizer.py
            '''
            if is_CosyVoice2:
                generator = cosyvoice.inference_cross_lingual(tts_text, prompt_speech_16k, speed=speed, stream=False,text_frontend=text_frontend)
            else:
                generator = cosyvoice.inference_cross_lingual(tts_text, prompt_speech_16k, stream=False,)
                
        elif mode == "Instruct2":
            '''
            inference_instruct2 params:
                tts_text: éœ€è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬ã€‚
                instruct_text: æŒ‡ä»¤æ–‡æœ¬ï¼Œç”¨äºå¼•å¯¼è¯­éŸ³åˆæˆçš„è¿‡ç¨‹ã€‚
                prompt_speech_16k: ä¸€ä¸ªå‚è€ƒè¯­éŸ³ä¿¡å·ï¼Œé‡‡æ ·ç‡ä¸º16kHzã€‚
                stream: æ˜¯å¦ä»¥æµçš„å½¢å¼è¾“å‡ºè¯­éŸ³ï¼Œé»˜è®¤ä¸ºFalseã€‚
                speed: ç”Ÿæˆè¯­éŸ³çš„é€Ÿåº¦ï¼Œ1.0è¡¨ç¤ºæ­£å¸¸é€Ÿåº¦ã€‚
                text_frontend: æ˜¯å¦ä½¿ç”¨æ–‡æœ¬å‰ç«¯å¤„ç†ï¼Œé»˜è®¤ä¸ºTrueã€‚
            '''
            generator = cosyvoice.inference_instruct2(tts_text, instruct_text, prompt_speech_16k, speed=speed, stream=False,text_frontend=text_frontend)
        elif mode == "SFT":
            generator = cosyvoice.inference_sft(tts_text, spk_id, stream=False)
        elif mode == "VC":
            generator = cosyvoice.inference_vc(source_speech_16k, prompt_speech_16k, stream=False)
        elif mode == "Instruct":
            generator = cosyvoice.inference_instruct(tts_text, spk_id, instruct_text, stream=False)

        if generator is not None:
            output_audio = get_output_data(generator)
            output_audio = {"waveform":output_audio, "sample_rate": cosyvoice.sample_rate}
        
        return output_audio, 

# from comfy_extras.nodes_audio import insert_or_replace_vorbis_comment
from comfy_extras.nodes_audio import SaveAudio as SA
class HJHCosyVoiceSaveAudio(SA):
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("output_path",)
    OUTPUT_IS_LIST =(True,)

    FUNCTION = "save"
    # def __init__(self):
    #     super().__init__()

    def save(self, audio, filename_prefix="ComfyUI", prompt=None, extra_pnginfo=None):
        # full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)

        result = super().save_audio(audio, filename_prefix, prompt, extra_pnginfo)
        # results.append({
        #     "filename": file,
        #     "subfolder": subfolder,
        #     "type": self.type
        # })
        ui_result = result["ui"]["audio"]
        paths = []
        for i in range(len(ui_result)):
            path = os.path.join(folder_paths.get_output_directory(),ui_result[i]["subfolder"], ui_result[i]["filename"])
            paths.append(path)

        result["result"] = paths,
        print("******************",result)
        return result


# Add custom API routes, using router
# from aiohttp import web
# from server import PromptServer

# @PromptServer.instance.routes.get("/hello")
# async def get_hello(request):
#     return web.json_response("hello")



NODE_CLASS_MAPPINGS = {
    "CosyVoiceModel": CosyVoiceModel,
    "CosyVoiceNode": CosyVoiceNode,
    "HJHCosyVoiceSaveAudio": HJHCosyVoiceSaveAudio,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "CosyVoiceModel": "HJH-CosyVoice - Load Model",
    "CosyVoiceNode": "HJH-CosyVoice - Generate Audio",
    "HJHCosyVoiceSaveAudio": "HJH-CosyVoice - Save Audio",
}